{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Embed Data and Upload to Azure Search\n\nThis notebook:\n1. Loads data from **CSV** or **Celonis EMS** (configurable below)\n2. Generates vector embeddings using Azure OpenAI\n3. Uploads documents to Azure Search\n4. Provides checkpoint/resume functionality for large datasets\n\n## Features:\n- ‚úÖ Automatic checkpointing every 250 rows\n- ‚úÖ Resume from checkpoint if interrupted\n- ‚úÖ Progress tracking\n- ‚úÖ Error handling\n- ‚úÖ CSV or Celonis data source"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Import utilities and load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress SSL warnings (needed for IP-based endpoints)\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path().absolute().parent))\n",
    "\n",
    "from utils import (\n",
    "    config,\n",
    "    load_csv,\n",
    "    format_datetime_column,\n",
    "    init_embedding_tracking,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    get_embedding,\n",
    "    upload_documents,\n",
    "    print_embedding_summary,\n",
    "    get_index_stats\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"   Index: {config.azure_search_index_name}\")\n",
    "print(f\"   CSV: {config.csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration - Customize for Your Project\n",
    "\n",
    "**Update these mappings to match your data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map CSV columns to Azure Search index fields\n",
    "FIELD_MAPPING = {\n",
    "    # CSV Column Name : Index Field Name\n",
    "    \"Id\": \"contract_item_id\",\n",
    "    \"SystemContractNumber\": \"contract_number\",\n",
    "    \"SystemContractItemNumber\": \"contract_item_number\",\n",
    "    \"ShortText\": \"item_text\",\n",
    "    \"Name\": \"vendor_name\",\n",
    "    \"NetUnitPrice\": \"unit_price\",\n",
    "    \"Currency\": \"currency\",\n",
    "    \"ValidityPeriodStartDate\": \"contract_start\",\n",
    "    \"ValidityPeriodEndDate\": \"contract_end\"\n",
    "}\n",
    "\n",
    "# Which CSV column contains the text to embed?\n",
    "TEXT_COLUMN_TO_EMBED = \"ShortText\"\n",
    "\n",
    "# Which columns contain dates that need formatting?\n",
    "DATETIME_COLUMNS = [\"ValidityPeriodStartDate\", \"ValidityPeriodEndDate\"]\n",
    "\n",
    "print(\"‚úÖ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2b. Data Source Selection\n\nChoose your data source: `\"csv\"` (default) or `\"celonis\"`.\n\nIf using Celonis, configure the PQL columns below and ensure your `.env` has the Celonis credentials set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ---- Choose data source ----\nDATA_SOURCE = \"csv\"  # Change to \"celonis\" to load from Celonis EMS\n\n# ---- Celonis PQL columns (only used when DATA_SOURCE = \"celonis\") ----\n# Each column \"name\" must match the keys in FIELD_MAPPING above.\nCELONIS_PQL_COLUMNS = [\n    {\"name\": \"Id\",                          \"query\": '\"o_celonis_ContractItem\".\"ID\"'},\n    {\"name\": \"SystemContractNumber\",        \"query\": '\"o_celonis_ContractItem\".\"SystemContractNumber\"'},\n    {\"name\": \"SystemContractItemNumber\",    \"query\": '\"o_celonis_ContractItem\".\"SystemContractItemNumber\"'},\n    {\"name\": \"ShortText\",                   \"query\": '\"o_celonis_ContractItem\".\"ShortText\"'},\n    {\"name\": \"Name\",                        \"query\": '\"o_celonis_ContractItem\".\"Name\"'},\n    {\"name\": \"NetUnitPrice\",                \"query\": '\"o_celonis_ContractItem\".\"NetUnitPrice\"'},\n    {\"name\": \"Currency\",                    \"query\": '\"o_celonis_ContractItem\".\"Currency\"'},\n    {\"name\": \"ValidityPeriodStartDate\",     \"query\": '\"o_celonis_ContractItem\".\"ValidityPeriodStartDate\"'},\n    {\"name\": \"ValidityPeriodEndDate\",       \"query\": '\"o_celonis_ContractItem\".\"ValidityPeriodEndDate\"'},\n]\n\nprint(f\"‚úÖ Data source: {DATA_SOURCE}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Load Data\n\nLoads from checkpoint (if resuming), Celonis (if selected), or CSV (default)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try to load from checkpoint first\ndf = load_checkpoint(config.checkpoint_file_path)\n\nif df is not None:\n    print(\"‚úÖ Resumed from checkpoint\\n\")\nelif DATA_SOURCE == \"celonis\":\n    from utils import load_celonis_data\n    config.validate_celonis()\n    print(\"üì° Loading data from Celonis EMS...\\n\")\n    df = load_celonis_data(columns=CELONIS_PQL_COLUMNS)\nelse:\n    print(\"‚ÑπÔ∏è  Loading from CSV...\\n\")\n    df = load_csv(config.csv_file_path)\n\nprint(f\"\\nüìä Dataset shape: {df.shape}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data\n",
    "\n",
    "Format datetime columns and initialize tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datetime columns for Azure\n",
    "print(\"üìÖ Formatting datetime columns...\")\n",
    "for col in DATETIME_COLUMNS:\n",
    "    if col in df.columns:\n",
    "        format_datetime_column(df, col)\n",
    "        print(f\"   ‚úì {col}\")\n",
    "\n",
    "# Initialize tracking columns\n",
    "df = init_embedding_tracking(df)\n",
    "\n",
    "print(\"\\n‚úÖ Data prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Embedding on One Row\n",
    "\n",
    "Before processing all data, test on a single row to verify everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample row\n",
    "test_row = df.iloc[0]\n",
    "test_text = test_row[TEXT_COLUMN_TO_EMBED]\n",
    "\n",
    "print(f\"üß™ Testing embedding on: '{test_text}'\\n\")\n",
    "\n",
    "# Generate embedding\n",
    "embedding = get_embedding(test_text)\n",
    "\n",
    "if embedding:\n",
    "    print(f\"‚úÖ Embedding generated successfully\")\n",
    "    print(f\"   Dimensions: {len(embedding)}\")\n",
    "    print(f\"   First 5 values: {embedding[:5]}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to generate embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Current Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_embedding_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process Small Batch (Test)\n",
    "\n",
    "Process just 5 rows as a test before running the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rows(df, max_rows=None, checkpoint_every=250):\n",
    "    \"\"\"Process rows: embed and upload\"\"\"\n",
    "    \n",
    "    # Filter to pending rows\n",
    "    pending = df[df[\"embedded_status\"] != \"success\"]\n",
    "    \n",
    "    if len(pending) == 0:\n",
    "        print(\"‚úÖ All rows already processed!\")\n",
    "        return\n",
    "    \n",
    "    # Limit if requested\n",
    "    if max_rows:\n",
    "        pending = pending.head(max_rows)\n",
    "        print(f\"‚öôÔ∏è  Processing {max_rows} rows (test mode)\\n\")\n",
    "    \n",
    "    total = len(pending)\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for i, (idx, row) in enumerate(pending.iterrows(), 1):\n",
    "        doc_id = row.get(\"Id\", f\"row_{idx}\")\n",
    "        \n",
    "        print(f\"‚Üí [{i}/{total}] {doc_id}...\", end=\" \")\n",
    "        \n",
    "        # 1. Generate embedding\n",
    "        text = row[TEXT_COLUMN_TO_EMBED]\n",
    "        embedding = get_embedding(text)\n",
    "        \n",
    "        if embedding is None:\n",
    "            print(\"‚ùå Embedding failed\")\n",
    "            df.at[idx, \"embedded_status\"] = \"failed\"\n",
    "            df.at[idx, \"embedded_error\"] = \"Embedding generation failed\"\n",
    "            df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "            fail_count += 1\n",
    "            continue\n",
    "        \n",
    "        # 2. Prepare document\n",
    "        doc = {}\n",
    "        for csv_col, index_field in FIELD_MAPPING.items():\n",
    "            if csv_col in row.index:\n",
    "                value = row[csv_col]\n",
    "                if pd.isna(value):\n",
    "                    value = None\n",
    "                doc[index_field] = value\n",
    "        \n",
    "        doc[\"embedding\"] = embedding\n",
    "        \n",
    "        # 3. Upload\n",
    "        try:\n",
    "            success = upload_documents([doc])\n",
    "            if success:\n",
    "                print(\"‚úÖ\")\n",
    "                df.at[idx, \"embedded_status\"] = \"success\"\n",
    "                df.at[idx, \"embedded_error\"] = \"\"\n",
    "                df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(\"‚ùå Upload failed\")\n",
    "                df.at[idx, \"embedded_status\"] = \"failed\"\n",
    "                df.at[idx, \"embedded_error\"] = \"Upload failed\"\n",
    "                df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "                fail_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {str(e)[:50]}\")\n",
    "            df.at[idx, \"embedded_status\"] = \"failed\"\n",
    "            df.at[idx, \"embedded_error\"] = str(e)[:2000]\n",
    "            df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Checkpoint periodically\n",
    "        processed = success_count + fail_count\n",
    "        if processed % checkpoint_every == 0:\n",
    "            save_checkpoint(df, config.checkpoint_file_path)\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(config.sleep_between_requests)\n",
    "    \n",
    "    # Final checkpoint\n",
    "    save_checkpoint(df, config.checkpoint_file_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Batch complete: {success_count} success, {fail_count} failed\")\n",
    "\n",
    "\n",
    "# Process 5 rows as a test\n",
    "process_rows(df, max_rows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process All Remaining Rows\n",
    "\n",
    "‚ö†Ô∏è **This will process ALL remaining rows**\n",
    "\n",
    "Remove `max_rows` parameter to process everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all rows (remove max_rows parameter)\n",
    "process_rows(df)  # or process_rows(df, max_rows=100) for another test batch\n",
    "\n",
    "# Show final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print_embedding_summary(df)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verify Upload - Check Index Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Checking index statistics...\\n\")\n",
    "get_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Search\n",
    "\n",
    "Try out all three search modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import text_search, vector_search, hybrid_search\n",
    "import json\n",
    "\n",
    "# Your search query\n",
    "QUERY = \"CHAIR\"\n",
    "\n",
    "def print_results(results, title):\n",
    "    \"\"\"Pretty print search results\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{title}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    if not results or \"value\" not in results:\n",
    "        print(\"No results found\")\n",
    "        return\n",
    "    \n",
    "    for i, doc in enumerate(results[\"value\"], 1):\n",
    "        score = doc.get(\"@search.score\", \"N/A\")\n",
    "        item_id = doc.get(\"contract_item_id\", \"N/A\")\n",
    "        item_text = doc.get(\"item_text\", \"N/A\")\n",
    "        vendor = doc.get(\"vendor_name\", \"N/A\")\n",
    "        price = doc.get(\"unit_price\", \"N/A\")\n",
    "        \n",
    "        print(f\"\\n{i}. {item_text}\")\n",
    "        print(f\"   Score: {score}\")\n",
    "        print(f\"   ID: {item_id}\")\n",
    "        print(f\"   Vendor: {vendor}\")\n",
    "        print(f\"   Price: {price}\")\n",
    "\n",
    "print(f\"üîç Searching for: '{QUERY}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Search (BM25 - Keyword Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = text_search(QUERY, top_k=3)\n",
    "print_results(results, \"Text Search (BM25)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Search (Semantic Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_search(QUERY, top_k=3)\n",
    "print_results(results, \"Vector Search (Semantic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search (Best of Both Worlds) - RECOMMENDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which fields to search for the BM25 leg\n",
    "search_fields = [\"item_text\", \"vendor_name\", \"contract_number\"]\n",
    "\n",
    "results = hybrid_search(\n",
    "    QUERY, \n",
    "    top_k=3,\n",
    "    search_fields=search_fields\n",
    ")\n",
    "print_results(results, \"Hybrid Search (Text + Vector with RRF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Your index is now populated with embedded data! You can:\n",
    "\n",
    "1. **Use the search script:** `python scripts/search_index.py --query \"your query\" --mode hybrid`\n",
    "2. **Integrate into your application:** Import the search functions from `utils`\n",
    "3. **Process more data:** Just run this notebook again with new CSV data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
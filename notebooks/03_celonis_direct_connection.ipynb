{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celonis Direct Connection - Extract, Embed & Upload\n",
    "\n",
    "Step-by-step guide for connecting to Celonis EMS, extracting data via PQL queries,\n",
    "generating embeddings, and uploading to Azure AI Search.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Celonis EMS account with API access\n",
    "- `pycelonis` installed: `pip install --extra-index-url=https://pypi.celonis.cloud/ pycelonis`\n",
    "- Azure credentials configured in `.env`\n",
    "- Celonis credentials configured in `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress SSL warnings (needed for IP-based endpoints)\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path().absolute().parent))\n",
    "\n",
    "from utils import (\n",
    "    config,\n",
    "    connect_celonis,\n",
    "    get_data_pool,\n",
    "    get_data_model,\n",
    "    list_data_pools,\n",
    "    list_data_models,\n",
    "    build_pql_query,\n",
    "    extract_dataframe,\n",
    "    format_datetime_column,\n",
    "    init_embedding_tracking,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    get_embedding,\n",
    "    upload_documents,\n",
    "    print_embedding_summary,\n",
    "    get_index_stats\n",
    ")\n",
    "\n",
    "print(\"\\u2705 Setup complete\")\n",
    "print(f\"   Index: {config.azure_search_index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Celonis\n",
    "\n",
    "Uses credentials from `.env` (CELONIS_BASE_URL, CELONIS_API_TOKEN, CELONIS_KEY_TYPE).\n",
    "You can also pass them directly to override."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.validate_celonis()\n",
    "celonis = connect_celonis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Data Pools & Models\n",
    "\n",
    "Use these cells to discover what's available in your Celonis environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data pools\n",
    "pools = list_data_pools(celonis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your data pool (uses CELONIS_DATA_POOL_NAME or _ID from .env)\n",
    "data_pool = get_data_pool(celonis)\n",
    "\n",
    "# List models in this pool\n",
    "models = list_data_models(data_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your data model (uses CELONIS_DATA_MODEL_NAME or _ID from .env)\n",
    "data_model = get_data_model(data_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build PQL Query\n",
    "\n",
    "Define the columns to extract. Each column needs:\n",
    "- `name`: The output column name (should match your FIELD_MAPPING keys)\n",
    "- `query`: The PQL expression to evaluate\n",
    "\n",
    "Customize the table and column names for your data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PQL columns - customize for your data model\n",
    "PQL_COLUMNS = [\n",
    "    {\"name\": \"Id\",                          \"query\": '\"o_celonis_ContractItem\".\"ID\"'},\n",
    "    {\"name\": \"SystemContractNumber\",        \"query\": '\"o_celonis_ContractItem\".\"SystemContractNumber\"'},\n",
    "    {\"name\": \"SystemContractItemNumber\",    \"query\": '\"o_celonis_ContractItem\".\"SystemContractItemNumber\"'},\n",
    "    {\"name\": \"ShortText\",                   \"query\": '\"o_celonis_ContractItem\".\"ShortText\"'},\n",
    "    {\"name\": \"Name\",                        \"query\": '\"o_celonis_ContractItem\".\"Name\"'},\n",
    "    {\"name\": \"NetUnitPrice\",                \"query\": '\"o_celonis_ContractItem\".\"NetUnitPrice\"'},\n",
    "    {\"name\": \"Currency\",                    \"query\": '\"o_celonis_ContractItem\".\"Currency\"'},\n",
    "    {\"name\": \"ValidityPeriodStartDate\",     \"query\": '\"o_celonis_ContractItem\".\"ValidityPeriodStartDate\"'},\n",
    "    {\"name\": \"ValidityPeriodEndDate\",       \"query\": '\"o_celonis_ContractItem\".\"ValidityPeriodEndDate\"'},\n",
    "]\n",
    "\n",
    "# Build the query (optional: add filters, ordering, limit)\n",
    "query = build_pql_query(\n",
    "    columns=PQL_COLUMNS,\n",
    "    # filters=['FILTER \"o_celonis_ContractItem\".\"Currency\" = \\'USD\\''],\n",
    "    # order_by=[{\"query\": '\"o_celonis_ContractItem\".\"ID\"', \"direction\": \"ASC\"}],\n",
    "    # distinct=True,\n",
    "    # limit=1000,\n",
    ")\n",
    "\n",
    "print(f\"Query built with {len(PQL_COLUMNS)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Data to DataFrame\n",
    "\n",
    "Execute the PQL query and preview the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_dataframe(data_model, query)\n",
    "\n",
    "print(f\"\\n\\ud83d\\udcca Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Field Mapping\n",
    "\n",
    "Map extracted column names to Azure Search index fields.\n",
    "Since PQL column `name` values match the CSV column names, the same mapping works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_MAPPING = {\n",
    "    \"Id\": \"contract_item_id\",\n",
    "    \"SystemContractNumber\": \"contract_number\",\n",
    "    \"SystemContractItemNumber\": \"contract_item_number\",\n",
    "    \"ShortText\": \"item_text\",\n",
    "    \"Name\": \"vendor_name\",\n",
    "    \"NetUnitPrice\": \"unit_price\",\n",
    "    \"Currency\": \"currency\",\n",
    "    \"ValidityPeriodStartDate\": \"contract_start\",\n",
    "    \"ValidityPeriodEndDate\": \"contract_end\"\n",
    "}\n",
    "\n",
    "TEXT_COLUMN_TO_EMBED = \"ShortText\"\n",
    "DATETIME_COLUMNS = [\"ValidityPeriodStartDate\", \"ValidityPeriodEndDate\"]\n",
    "\n",
    "# Verify all mapping keys exist in the DataFrame\n",
    "missing = [k for k in FIELD_MAPPING if k not in df.columns]\n",
    "if missing:\n",
    "    print(f\"\\u26a0\\ufe0f  Warning: columns missing from data: {missing}\")\n",
    "else:\n",
    "    print(\"\\u2705 All field mapping columns present in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Data\n",
    "\n",
    "Format datetime columns and initialize embedding tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ud83d\\udcc5 Formatting datetime columns...\")\n",
    "for col in DATETIME_COLUMNS:\n",
    "    if col in df.columns:\n",
    "        format_datetime_column(df, col)\n",
    "        print(f\"   \\u2713 {col}\")\n",
    "\n",
    "df = init_embedding_tracking(df)\n",
    "print(\"\\n\\u2705 Data prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Embedding on One Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = df.iloc[0]\n",
    "test_text = test_row[TEXT_COLUMN_TO_EMBED]\n",
    "\n",
    "print(f\"\\ud83e\\uddea Testing embedding on: '{test_text}'\\n\")\n",
    "\n",
    "embedding = get_embedding(test_text)\n",
    "\n",
    "if embedding:\n",
    "    print(f\"\\u2705 Embedding generated successfully\")\n",
    "    print(f\"   Dimensions: {len(embedding)}\")\n",
    "    print(f\"   First 5 values: {embedding[:5]}\")\n",
    "else:\n",
    "    print(\"\\u274c Failed to generate embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process & Upload\n",
    "\n",
    "First test with a small batch, then process all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rows(df, max_rows=None, checkpoint_every=250):\n",
    "    \"\"\"Process rows: embed and upload\"\"\"\n",
    "    pending = df[df[\"embedded_status\"] != \"success\"]\n",
    "\n",
    "    if len(pending) == 0:\n",
    "        print(\"\\u2705 All rows already processed!\")\n",
    "        return\n",
    "\n",
    "    if max_rows:\n",
    "        pending = pending.head(max_rows)\n",
    "        print(f\"\\u2699\\ufe0f  Processing {max_rows} rows (test mode)\\n\")\n",
    "\n",
    "    total = len(pending)\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    for i, (idx, row) in enumerate(pending.iterrows(), 1):\n",
    "        doc_id = row.get(\"Id\", f\"row_{idx}\")\n",
    "        print(f\"\\u2192 [{i}/{total}] {doc_id}...\", end=\" \")\n",
    "\n",
    "        text = row[TEXT_COLUMN_TO_EMBED]\n",
    "        embedding = get_embedding(text)\n",
    "\n",
    "        if embedding is None:\n",
    "            print(\"\\u274c Embedding failed\")\n",
    "            df.at[idx, \"embedded_status\"] = \"failed\"\n",
    "            df.at[idx, \"embedded_error\"] = \"Embedding generation failed\"\n",
    "            df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        doc = {}\n",
    "        for csv_col, index_field in FIELD_MAPPING.items():\n",
    "            if csv_col in row.index:\n",
    "                value = row[csv_col]\n",
    "                if pd.isna(value):\n",
    "                    value = None\n",
    "                doc[index_field] = value\n",
    "        doc[\"embedding\"] = embedding\n",
    "\n",
    "        try:\n",
    "            success = upload_documents([doc])\n",
    "            if success:\n",
    "                print(\"\\u2705\")\n",
    "                df.at[idx, \"embedded_status\"] = \"success\"\n",
    "                df.at[idx, \"embedded_error\"] = \"\"\n",
    "                df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(\"\\u274c Upload failed\")\n",
    "                df.at[idx, \"embedded_status\"] = \"failed\"\n",
    "                df.at[idx, \"embedded_error\"] = \"Upload failed\"\n",
    "                df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "                fail_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"\\u274c {str(e)[:50]}\")\n",
    "            df.at[idx, \"embedded_status\"] = \"failed\"\n",
    "            df.at[idx, \"embedded_error\"] = str(e)[:2000]\n",
    "            df.at[idx, \"embedded_at\"] = datetime.now(timezone.utc)\n",
    "            fail_count += 1\n",
    "\n",
    "        processed = success_count + fail_count\n",
    "        if processed % checkpoint_every == 0:\n",
    "            save_checkpoint(df, config.checkpoint_file_path)\n",
    "\n",
    "        time.sleep(config.sleep_between_requests)\n",
    "\n",
    "    save_checkpoint(df, config.checkpoint_file_path)\n",
    "    print(f\"\\n\\u2705 Batch complete: {success_count} success, {fail_count} failed\")\n",
    "\n",
    "\n",
    "# Test with 5 rows first\n",
    "process_rows(df, max_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all remaining rows\n",
    "process_rows(df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print_embedding_summary(df)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify & Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ud83d\\udcca Checking index statistics...\\n\")\n",
    "get_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import text_search, vector_search, hybrid_search\n",
    "\n",
    "QUERY = \"CHAIR\"\n",
    "\n",
    "def print_results(results, title):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(title)\n",
    "    print('=' * 80)\n",
    "    if not results or \"value\" not in results:\n",
    "        print(\"No results found\")\n",
    "        return\n",
    "    for i, doc in enumerate(results[\"value\"], 1):\n",
    "        print(f\"\\n{i}. {doc.get('item_text', 'N/A')}\")\n",
    "        print(f\"   Score: {doc.get('@search.score', 'N/A')}\")\n",
    "        print(f\"   Vendor: {doc.get('vendor_name', 'N/A')}\")\n",
    "\n",
    "print(f\"\\ud83d\\udd0d Searching for: '{QUERY}'\")\n",
    "\n",
    "results = hybrid_search(QUERY, top_k=3, search_fields=[\"item_text\", \"vendor_name\"])\n",
    "print_results(results, \"Hybrid Search (Text + Vector with RRF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: PQL Reference\n",
    "\n",
    "### Filters\n",
    "```python\n",
    "filters = [\n",
    "    'FILTER \"Table\".\"Status\" = \\'Active\\'',\n",
    "    'FILTER \"Table\".\"Amount\" > 1000',\n",
    "    'FILTER \"Table\".\"Date\" >= \\'2024-01-01\\'',\n",
    "]\n",
    "```\n",
    "\n",
    "### Ordering\n",
    "```python\n",
    "order_by = [\n",
    "    {\"query\": '\"Table\".\"Date\"', \"direction\": \"DESC\"},\n",
    "    {\"query\": '\"Table\".\"Name\"', \"direction\": \"ASC\"},\n",
    "]\n",
    "```\n",
    "\n",
    "### Pagination / Limits\n",
    "```python\n",
    "query = build_pql_query(columns=PQL_COLUMNS, limit=5000)\n",
    "```\n",
    "\n",
    "### Distinct\n",
    "```python\n",
    "query = build_pql_query(columns=PQL_COLUMNS, distinct=True)\n",
    "```\n",
    "\n",
    "### Aggregations\n",
    "```python\n",
    "columns = [\n",
    "    {\"name\": \"vendor\", \"query\": '\"Table\".\"VendorName\"'},\n",
    "    {\"name\": \"total\", \"query\": 'SUM(\"Table\".\"Amount\")'},\n",
    "    {\"name\": \"count\", \"query\": 'COUNT(\"Table\".\"ID\")'},\n",
    "]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
